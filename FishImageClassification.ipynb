{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "mDgbUHAGgjLW",
        "-Kee-DAl2viO"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -Multiclass Fish Image Classification\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification/supervised\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member  -** Dhruv Tamirisa\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiclass Fish Image Classification is a deep learning capstone project aimed at building a robust automated system to classify fish species from images. With increasing global emphasis on sustainable fisheries, ecological monitoring, and food supply tracking, rapid and accurate fish identification has immense value in both scientific and commercial domains.\n",
        "\n",
        "The core objective of this project is to design, train, and deploy a high-performance image classification model that can differentiate various fish species present in a given dataset. The solution leverages two main approaches:\n",
        "\n",
        "Custom-built Convolutional Neural Network (CNN): Constructed from scratch using TensorFlow/Keras, this model learns fish-specific features directly from the training images, allowing us to understand the baseline power of standard deep neural architectures.\n",
        "\n",
        "Transfer Learning: Advanced pre-trained CNNs (such as ResNet, VGG, EfficientNet, etc.), previously trained on large and diverse datasets like ImageNet, are fine-tuned on our fish image dataset. This approach exploits learned general image features and adapts them to the nuances of fish classification, improving convergence speed and accuracy, especially with limited data.\n",
        "\n",
        "The project pipeline begins with data understanding and exploration. The provided dataset is well-structured: images are arranged in class-specific folders across 'train', 'validation', and 'test' splits. Through exploratory data analysis, the class balance, image quality, and potential outliers are visualized and insights are gathered to guide preprocessing.\n",
        "\n",
        "Data preprocessing and augmentation are crucial to ensure model generalization. Techniques like random rotation, zooming, flipping, and scaling address dataset limitations and simulate real-world imaging variability. The data generators stream augmented image batches efficiently to the model during training.\n",
        "\n",
        "After model development and validation, the best-performing model—selected based on validation accuracy and robust evaluation metrics (like accuracy, precision, recall, confusion matrix)—is saved in a portable format (.h5 or .pkl). Comprehensive model evaluation is performed via visualization of learning curves, confusion matrices, and other EDA charts.\n",
        "\n",
        "One of the cornerstone deliverables is the deployment of the trained model as an interactive web application using Streamlit. This application allows end-users (biologists, seafood industry workers, citizen scientists) to upload fish images and instantly obtain predicted species, thereby enabling powerful real-world impact.\n",
        "\n",
        "The workflow is complemented by a well-documented public GitHub repository, including reproducible code, requirements, Jupyter notebooks, and deployment scripts/contact.\n",
        "\n",
        "Key skills demonstrated include:\n",
        "\n",
        "Deep Learning model design and training (TensorFlow/Keras)\n",
        "\n",
        "Efficient data pipeline construction with ImageDataGenerator\n",
        "\n",
        "Data visualization and exploratory analysis (EDA)\n",
        "\n",
        "Transfer learning and model selection\n",
        "\n",
        "Model persistence (saving, loading) and production readiness\n",
        "\n",
        "Deployment of ML models via Streamlit for real-time inference\n",
        "\n",
        "Collaborative documentation for reproducibility\n",
        "\n",
        "In summary, this project delivers a full-stack, real-world scalable image classification system, serving as an invaluable template for future AI-driven ecological and industrial applications requiring robust visual recognition."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/DhruvTamirisa/Fish-Species-Image-Classification-Deep-Learning-"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Design a scalable deep learning solution to accurately classify images of different fish species in a multiclass setting. The provided dataset consists of labeled images for several fish types, split into training, validation, and test sets.\n",
        "\n",
        "The project involves:\n",
        "\n",
        "Building a custom CNN model from scratch to serve as baseline.\n",
        "\n",
        "Leveraging transfer learning with pre-trained architectures to maximize accuracy.\n",
        "\n",
        "Applying rigorous data preprocessing and augmentation to handle image variability.\n",
        "\n",
        "Systematically evaluating model performance and selecting the highest-accuracy model.\n",
        "\n",
        "Persisting the best model for future use (.h5 or .pkl).\n",
        "\n",
        "Deploying the trained model via a Streamlit web application that allows users to upload fish photos and get immediate predicted class labels.\n",
        "\n",
        "The solution should be user-friendly, deployment-ready, and able to generalize well to new, unseen images. The final product will enable stakeholders to obtain real-time predictions, supporting use cases in ecological research, seafood supply monitoring, and educational outreach."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir('/content'))\n"
      ],
      "metadata": {
        "id": "kQBt5ttYKAhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm /content/fish.zip"
      ],
      "metadata": {
        "id": "RAMu-NuRmAeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -oq /content/fish.zip -d /content/"
      ],
      "metadata": {
        "id": "4oUBOgz7nasc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = '/content/images.cv_jzk6llhf18tm3k0kyttxz/data/train'\n",
        "val_dir   = '/content/images.cv_jzk6llhf18tm3k0kyttxz/data/val'\n",
        "test_dir  = '/content/images.cv_jzk6llhf18tm3k0kyttxz/data/test'\n"
      ],
      "metadata": {
        "id": "W_rVMttWKAP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "train_dir = '/content/images.cv_jzk6llhf18tm3k0kyttxz/data/train'\n",
        "val_dir   = '/content/images.cv_jzk6llhf18tm3k0kyttxz/data/val'\n",
        "test_dir  = '/content/images.cv_jzk6llhf18tm3k0kyttxz/data/test'\n",
        "\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=30,\n",
        "    zoom_range=0.2,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_gen = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "val_gen = val_test_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "test_gen = val_test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "fxxjfRWlJ_4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "batch_images, batch_labels = next(train_gen)\n",
        "plt.figure(figsize=(12,6))\n",
        "class_names = list(train_gen.class_indices.keys())\n",
        "for i in range(8):\n",
        "    plt.subplot(2,4,i+1)\n",
        "    plt.imshow(batch_images[i])\n",
        "    plt.title(class_names[np.argmax(batch_labels[i])])\n",
        "    plt.axis('off')\n",
        "plt.suptitle(\"Sample Images From Training Data\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Class distribution in the training set:\")\n",
        "for cls in sorted(os.listdir(train_dir)):\n",
        "    count = len(os.listdir(os.path.join(train_dir, cls)))\n",
        "    print(f\"{cls}: {count} images\")\n",
        "print(\"\\nTotal images in train set:\", sum([len(os.listdir(os.path.join(train_dir, c))) for c in os.listdir(train_dir)]))\n"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f\"Number of classes: {train_gen.num_classes}\")\n",
        "print(\"Class index mapping:\", train_gen.class_indices)\n",
        "print(\"Image shape:\", train_gen.image_shape)\n",
        "print(\"Batch size:\", train_gen.batch_size)\n"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_files = []\n",
        "for cls in os.listdir(train_dir):\n",
        "    all_files.extend(os.listdir(os.path.join(train_dir, cls)))\n",
        "\n",
        "print(\"Total files:\", len(all_files))\n",
        "print(\"Unique filenames:\", len(set(all_files)))\n",
        "if len(all_files) > len(set(all_files)):\n",
        "    print(\"Warning: Some file names are duplicated (may not imply image duplication).\")\n",
        "else:\n",
        "    print(\"No duplicate file names found in training set.\")\n"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "missing_classes = []\n",
        "for cls in os.listdir(train_dir):\n",
        "    if len(os.listdir(os.path.join(train_dir, cls))) == 0:\n",
        "        missing_classes.append(cls)\n",
        "if missing_classes:\n",
        "    print(\"Empty folders/classes found:\", missing_classes)\n",
        "else:\n",
        "    print(\"No missing data or empty class folders in train set.\")\n"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# For image folders, plot class distribution to spot potential missing (underrepresented) classes\n",
        "class_counts = [len(os.listdir(os.path.join(train_dir,cls))) for cls in sorted(os.listdir(train_dir))]\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(x=sorted(os.listdir(train_dir)), y=class_counts)\n",
        "plt.xticks(rotation=60)\n",
        "plt.ylabel(\"Image Count\")\n",
        "plt.title(\"Class Distribution in Train Set\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset consists of fish images grouped by class (species/type) in separate folders within 'train', 'val', and 'test' directories. Each image is labeled implicitly by its folder. The dataset is suitable for multiclass image classification and seems well-organized for Keras/TensorFlow workflows.\n",
        "There are no missing values or null entries since the data is composed of images stored in a folder structure, not a table. Initial inspection shows a variety of classes (fish types), and it's important to check class balance. All images require preprocessing and resizing before model training."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For an image dataset, \"columns\" are essentially the class names (folders)\n",
        "class_names = list(train_gen.class_indices.keys())\n",
        "print(\"Fish classes (labels):\", class_names)\n"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Input variable:** Image of a fish, stored as a JPEG (RGB, typically 224x224 pixels after resizing).  \n",
        "**Target variable:** Fish class label, determined by the folder name containing the image (each folder is a species/type)."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_classes = os.listdir(train_dir)\n",
        "print(\"Unique fish classes:\", unique_classes)\n",
        "print(\"Total unique classes:\", len(unique_classes))\n",
        "for cls in unique_classes:\n",
        "    print(f\"{cls}: {len(os.listdir(os.path.join(train_dir, cls)))} images\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#most preprocessing is handled during data loading,\n",
        "from PIL import Image\n",
        "\n",
        "problem_files = []\n",
        "for cls in unique_classes:\n",
        "    for fname in os.listdir(os.path.join(train_dir, cls)):\n",
        "        fpath = os.path.join(train_dir, cls, fname)\n",
        "        try:\n",
        "            img = Image.open(fpath)\n",
        "            img.verify()  # Checks for image corruption\n",
        "        except Exception as e:\n",
        "            problem_files.append(fpath)\n",
        "if problem_files:\n",
        "    print(\"Found corrupt images:\", problem_files)\n",
        "else:\n",
        "    print(\"All images opened successfully; no corrupt files found.\")\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensured all image files could be opened and are not corrupt.\n",
        "\n",
        "Verified the dataset is organized with one folder per fish class, with image files inside.\n",
        "\n",
        "Checked class label names and confirmed the number of classes.\n",
        "\n",
        "Counted images per class, which helps identify class imbalance issues.\n",
        "\n",
        "All further resizing, rescaling, and augmentation is handled automatically using Keras' ImageDataGenerator pipeline, so no manual pre-processing is required on disk.\n",
        "\n",
        "Insights:\n",
        "The dataset is well-structured for deep learning image classification. If any class has a much lower image count, augment those during training for better balance. No errors or missing files, so the dataset is \"analysis-ready\""
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart 1: Class Distribution (Bar Plot)"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes = sorted(os.listdir(train_dir))\n",
        "img_counts = [len(os.listdir(os.path.join(train_dir, cls))) for cls in classes]\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.barplot(x=classes, y=img_counts)\n",
        "plt.xticks(rotation=60)\n",
        "plt.title('Class Distribution in Training Set')\n",
        "plt.xlabel('Fish Classes')\n",
        "plt.ylabel('Number of Images')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visualize how balanced the classes are in the training data. Class imbalance can lead to biased models and poor generalization."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows which classes are overrepresented or underrepresented"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. Seeing class imbalance can guide data augmentation or re-sampling strategies to ensure the model predicts all classes well."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart 2: Sample Images Grid (Image Panel)"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "plt.figure(figsize=(15,5))\n",
        "for idx, cls in enumerate(classes[:6]):\n",
        "    img_file = os.listdir(os.path.join(train_dir, cls))[0]\n",
        "    img_path = os.path.join(train_dir, cls, img_file)\n",
        "    img = load_img(img_path, target_size=(224, 224))\n",
        "    plt.subplot(1,6,idx+1)\n",
        "    plt.imshow(img)\n",
        "    plt.title(cls)\n",
        "    plt.axis('off')\n",
        "plt.suptitle('Sample Images from Six Fish Classes')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visually showcase the quality, appearance, and variation among fish classes in the dataset."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirms diversity of species and highlights image quality or potential anomalies."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes—highlights issues (if any) and demonstrates model readiness to stakeholders."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart 3: Train vs Validation Class Distribution (Grouped Bar Plot)\n"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_classes = sorted(os.listdir(val_dir))\n",
        "train_counts = [len(os.listdir(os.path.join(train_dir, cls))) for cls in classes]\n",
        "val_counts = [len(os.listdir(os.path.join(val_dir, cls))) for cls in val_classes]\n",
        "\n",
        "import numpy as np\n",
        "x = np.arange(len(classes))\n",
        "bar_width = 0.4\n",
        "\n",
        "plt.figure(figsize=(14,6))\n",
        "plt.bar(x - bar_width/2, train_counts, width=bar_width, label='Train')\n",
        "plt.bar(x + bar_width/2, val_counts, width=bar_width, label='Validation')\n",
        "plt.xlabel('Classes')\n",
        "plt.ylabel('Images')\n",
        "plt.xticks(x, classes, rotation=60)\n",
        "plt.title('Train vs Validation Class Distributions')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compare class balance between training and validation datasets—important for fair performance evaluation."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shows if class balance is similar across splits or if any validation classes are missing/underrepresented."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensures validation is a reliable proxy for model testing; prompts corrections if splits are imbalanced."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart 4: Image Size Distribution (Histogram)"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "widths, heights = [], []\n",
        "for cls in classes:\n",
        "    class_dir = os.path.join(train_dir, cls)\n",
        "    for fname in os.listdir(class_dir):\n",
        "        try:\n",
        "            with Image.open(os.path.join(class_dir, fname)) as img:\n",
        "                widths.append(img.width)\n",
        "                heights.append(img.height)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "plt.figure(figsize=(14,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.hist(widths, bins=30, color='skyblue')\n",
        "plt.title('Image Width Distribution')\n",
        "plt.xlabel('Width (pixels)')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.hist(heights, bins=30, color='salmon')\n",
        "plt.title('Image Height Distribution')\n",
        "plt.xlabel('Height (pixels)')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand the distribution of image sizes, which affects preprocessing decisions and model input shapes."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reveals if images are uniform in size or if resizing/standardization is needed."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensures consistent model input and prevents errors during training/deployment."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart 5: Histogram of Image Aspect Ratios\n"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "aspect_ratios = []\n",
        "for cls in classes:\n",
        "    class_dir = os.path.join(train_dir, cls)\n",
        "    for fname in os.listdir(class_dir):\n",
        "        try:\n",
        "            with Image.open(os.path.join(class_dir, fname)) as img:\n",
        "                aspect_ratio = img.width / img.height\n",
        "                aspect_ratios.append(aspect_ratio)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.hist(aspect_ratios, bins=30, color='green')\n",
        "plt.title('Image Aspect Ratio Distribution')\n",
        "plt.xlabel('Width / Height')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check if images have consistent aspect ratios or if there is variation, which may affect resizing and preprocessing."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can determine if standard resizing might distort some classes or if the dataset is already uniform."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensures image reshaping does not harm class distinguishability, supporting robust predictions. Detecting aspect ratio outliers prevents negative impact (e.g., distorted images could confuse the model)."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart 6: Class Distribution in Test Set"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_classes = sorted(os.listdir(test_dir))\n",
        "test_counts = [len(os.listdir(os.path.join(test_dir, cls))) for cls in test_classes]\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.barplot(x=test_classes, y=test_counts, color='gray')\n",
        "plt.xticks(rotation=60)\n",
        "plt.title('Class Distribution in Test Set')\n",
        "plt.xlabel('Fish Classes')\n",
        "plt.ylabel('Number of Images')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visualize the class balance in the test set, making sure the test split is representative."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It highlights if any class is missing or underrepresented in the test set."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Balanced test class distribution is crucial for fair and accurate model evaluation, supporting robust deployment. If unbalanced, reported metrics could misrepresent performance, which could hurt business trust."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart 7: Average Image Brightness by Class"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "avg_brightness = []\n",
        "for cls in classes:\n",
        "    class_dir = os.path.join(train_dir, cls)\n",
        "    bright_vals = []\n",
        "    for fname in os.listdir(class_dir)[:10]:  # limit to 10 images/class for speed\n",
        "        try:\n",
        "            with Image.open(os.path.join(class_dir, fname)) as img:\n",
        "                img = img.convert('L')\n",
        "                bright_vals.append(np.array(img).mean())\n",
        "        except:\n",
        "            continue\n",
        "    avg_brightness.append(np.mean(bright_vals))\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "sns.barplot(x=classes, y=avg_brightness, palette='viridis')\n",
        "plt.xticks(rotation=60)\n",
        "plt.ylabel('Average Brightness')\n",
        "plt.title('Average Image Brightness by Class')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check for systematic exposure or lighting issues across classes."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It reveals classes that may be consistently brighter or darker, pointing to potential imaging biases."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unusual brightness might harm class separability or suggest need for standardization. Addressing this could improve model reliability in real-world use."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart 8: Number of Images per Class with Log Scale (Long Tail Visual)"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "sns.barplot(x=classes, y=img_counts)\n",
        "plt.yscale('log')\n",
        "plt.xticks(rotation=60)\n",
        "plt.title('Number of Images per Class (Log Scale)')\n",
        "plt.xlabel('Fish Classes')\n",
        "plt.ylabel('Number of Images (Log Scale)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To detect and highlight long-tail class imbalance that may not be noticeable on a linear scale.\n"
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shows if a few classes have far more images, which is a warning for potential model bias towards majority classes."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identifying long-tail imbalance guides the need for augmentation or re-weighting in training, ultimately leading to fairer and more effective deployment."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart 9: Boxplot of Number of Images per Class"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes = sorted(os.listdir(train_dir))\n",
        "img_counts = [len(os.listdir(os.path.join(train_dir, cls))) for cls in classes]\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.boxplot(data=img_counts, color='orange')\n",
        "plt.title('Boxplot of Number of Images per Class')\n",
        "plt.ylabel('Image Count per Class')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visualize the spread and outliers of image counts across all classes, making it easy to see imbalanced or rare classes."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The boxplot shows the median, quartiles, and outlier classes with very high or low image counts"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identifying outlier classes early allows for strategic augmentation or class weighting, improving model generalization and fairness."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart 10: Pie Chart of Class Proportions in Train Set"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "plt.pie(img_counts, labels=classes, autopct='%1.1f%%', startangle=140, textprops={'fontsize': 8})\n",
        "plt.title('Proportion of Images per Class')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pie chart gives a fast, intuitive sense of the class balance and whether any classes dominate"
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shows dominant and minor classes at a glance, visually confirming or refuting balance."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having a sense of class proportions helps anticipate model bias and guides balancing methods such as augmentation or sampling."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart 11: Cumulative Line Plot of Image Counts by Class"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_counts = np.sort(img_counts)[::-1]\n",
        "cum_percent = np.cumsum(sorted_counts) / np.sum(sorted_counts) * 100\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(range(1,len(cum_percent)+1), cum_percent, marker='o')\n",
        "plt.title('Cumulative % of Images vs. Number of Classes')\n",
        "plt.xlabel('Number of Top Classes')\n",
        "plt.ylabel('Cumulative Percentage (%)')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To demonstrate the “long tail” effect and how many classes account for the majority of training data."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shows if most images are concentrated in a few classes, which can induce class imbalance issues.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Highlights the need to invest in underrepresented classes to support broad and reliable model accuracy."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart 12: Bar Plot of Top 7 Classes by Image Count"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "count_data = dict(zip(classes, img_counts))\n",
        "top_classes = sorted(count_data, key=count_data.get, reverse=True)[:7]\n",
        "top_counts = [count_data[c] for c in top_classes]\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(x=top_classes, y=top_counts, palette='magma')\n",
        "plt.title('Top 7 Classes with Most Images')\n",
        "plt.xlabel('Fish Classes')\n",
        "plt.ylabel('Image Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Focusing on top classes highlights where model performance may be easiest to improve due to abundant data."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shows which classes could drive the model’s main accuracy, and if overfitting to “majority classes” is a risk."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enables resource focus (augmentation, quality checks) on both dominant and rare classes for strategic improvement."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart 13: Heatmap of Image Width, Height, and Brightness Correlations"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_widths, avg_heights, avg_brightness = [], [], []\n",
        "for cls in classes:\n",
        "    ws, hs, bs = [], [], []\n",
        "    class_dir = os.path.join(train_dir, cls)\n",
        "    for fname in os.listdir(class_dir)[:10]:  # sample 10 images for speed\n",
        "        try:\n",
        "            img = Image.open(os.path.join(class_dir, fname))\n",
        "            ws.append(img.width)\n",
        "            hs.append(img.height)\n",
        "            img_gray = img.convert('L')\n",
        "            bs.append(np.array(img_gray).mean())\n",
        "        except:\n",
        "            continue\n",
        "    avg_widths.append(np.mean(ws))\n",
        "    avg_heights.append(np.mean(hs))\n",
        "    avg_brightness.append(np.mean(bs))\n",
        "\n",
        "df_metrics = pd.DataFrame({\n",
        "    'Width': avg_widths,\n",
        "    'Height': avg_heights,\n",
        "    'Brightness': avg_brightness\n",
        "}, index=classes)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(df_metrics.corr(), annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Heatmap (Width, Height, Brightness)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check if image size and brightness are correlated across classes, which may prompt normalization or standardization."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can reveal if some classes tend to have larger/brighter images, suggesting collection or processing biases."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Addressing such biases helps ensure fairness and accuracy, and prevents model overfitting to spurious features."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Violin Plot of Brightness Distribution per Class"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a long-form df for per-image brightnesses\n",
        "brightness_long = {'Class': [], 'Brightness': []}\n",
        "for cls in classes:\n",
        "    class_dir = os.path.join(train_dir, cls)\n",
        "    for fname in os.listdir(class_dir)[:15]:  # sample subset for speed\n",
        "        try:\n",
        "            img = Image.open(os.path.join(class_dir, fname)).convert('L')\n",
        "            brightness_long['Class'].append(cls)\n",
        "            brightness_long['Brightness'].append(np.array(img).mean())\n",
        "        except:\n",
        "            continue\n",
        "brightness_df = pd.DataFrame(brightness_long)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.violinplot(x='Class', y='Brightness', data=brightness_df, inner='quartile')\n",
        "plt.xticks(rotation=60)\n",
        "plt.title('Violin Plot: Brightness Distribution per Fish Class')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A violin plot conveys both the median, spread, and the full shape of distributions—here for brightness—across all classes, revealing within-class variation and outliers."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see if certain classes are systematically brighter or darker, if classes have wide or narrow brightness variability, and where outliers are most common."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pairplot of Average Width, Height, and Brightness per Class"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dataframe\n",
        "classes = sorted(os.listdir(train_dir))\n",
        "metrics = {'Class': [], 'Width': [], 'Height': [], 'Brightness': []}\n",
        "for cls in classes:\n",
        "    class_dir = os.path.join(train_dir, cls)\n",
        "    widths, heights, brightnesses = [], [], []\n",
        "    for fname in os.listdir(class_dir)[:15]:\n",
        "        try:\n",
        "            img = Image.open(os.path.join(class_dir, fname))\n",
        "            widths.append(img.width)\n",
        "            heights.append(img.height)\n",
        "            gray = img.convert('L')\n",
        "            brightnesses.append(np.array(gray).mean())\n",
        "        except:\n",
        "            continue\n",
        "    if widths and heights and brightnesses:\n",
        "        metrics['Class'].append(cls)\n",
        "        metrics['Width'].append(np.mean(widths))\n",
        "        metrics['Height'].append(np.mean(heights))\n",
        "        metrics['Brightness'].append(np.mean(brightnesses))\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics)\n",
        "\n",
        "sns.pairplot(metrics_df[['Width', 'Height', 'Brightness']])\n",
        "plt.suptitle('Pairplot of Avg Width, Height, and Brightness per Class', y=1.01)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pairplot visualizes all pairwise relationships between features and highlights natural clusters or anomalies that wouldn’t be obvious in a single chart."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can spot if classes group together based on physical characteristics, or if there's significant overlap among the feature distributions. Outliers, trends, or correlated features show up in the off-diagonal scatterplots."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no significant difference in average image brightness between the fish classes.\n",
        "\n",
        "Null Hypothesis (H₀):\n",
        "Mean brightness is the same for all fish classes.\n",
        "\n",
        "Alternative Hypothesis (H₁):\n",
        "At least one class differs in average brightness."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import f_oneway\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "train_dir = '/content/images.cv_jzk6llhf18tm3k0kyttxz/data/train'\n",
        "classes = sorted(os.listdir(train_dir))\n",
        "\n",
        "brightness_data = {}\n",
        "for cls in classes:\n",
        "    brightness_list = []\n",
        "    class_path = os.path.join(train_dir, cls)\n",
        "    for fname in os.listdir(class_path)[:20]:  # use a sample of 20 for speed\n",
        "        try:\n",
        "            img = Image.open(os.path.join(class_path, fname)).convert('L')\n",
        "            brightness_list.append(np.array(img).mean())\n",
        "        except:\n",
        "            pass\n",
        "    brightness_data[cls] = brightness_list\n",
        "\n",
        "# Prepare lists for statistical test\n",
        "groups = [vals for vals in brightness_data.values() if len(vals) > 0]\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "F_stat, p_val = f_oneway(*groups)\n",
        "print(f\"ANOVA test result: F = {F_stat:.3f}, p = {p_val:.5f}\")\n",
        "\n",
        "# Interpret\n",
        "alpha = 0.05\n",
        "if p_val < alpha:\n",
        "    print(\"Reject null hypothesis: There is a significant brightness difference between fish classes.\")\n",
        "else:\n",
        "    print(\"Fail to reject null hypothesis: No significant difference in brightness between classes.\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-way ANOVA (Analysis of Variance):\n",
        "\n",
        "Compares means of more than two groups (fish classes here) to check if at least one mean is significantly different."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANOVA is appropriate when comparing means across multiple groups (fish classes) for a continuous variable (brightness).\n",
        "\n",
        "Image brightness is a numeric value, and each class is independent."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mean image width is equal for 'fish sea food sea bass' and 'fish sea food striped red mullet'.\n",
        "\n",
        "Null Hypothesis (H₀):\n",
        "μ\n",
        "sea bass,width\n",
        "=\n",
        "μ\n",
        "striped red mullet,width\n",
        "\n",
        "Alternate Hypothesis (H₁):\n",
        "μ\n",
        "sea bass,width\n",
        "≠\n",
        "μ\n",
        "striped red mullet,width\n",
        "\n"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ttest_ind\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "train_dir = '/content/images.cv_jzk6llhf18tm3k0kyttxz/data/train'\n",
        "class1 = '/content/images.cv_jzk6llhf18tm3k0kyttxz/data/train/fish sea_food sea_bass'\n",
        "class2 = '/content/images.cv_jzk6llhf18tm3k0kyttxz/data/train/fish sea_food red_mullet'\n",
        "\n",
        "widths1 = [Image.open(os.path.join(train_dir, class1, f)).width for f in os.listdir(os.path.join(train_dir, class1))]\n",
        "widths2 = [Image.open(os.path.join(train_dir, class2, f)).width for f in os.listdir(os.path.join(train_dir, class2))]\n",
        "\n",
        "stat, p_val = ttest_ind(widths1, widths2)\n",
        "print(f\"t={stat:.2f}, p={p_val:.4f}\")\n",
        "\n",
        "if p_val < 0.05:\n",
        "    print(\"Reject H₀: Widths are significantly different.\")\n",
        "else:\n",
        "    print(\"Fail to reject H₀: No significant width difference.\")\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two-sample t-test (independent t-test, because these class samples are independent)."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A t-test is ideal for comparing means between two independent groups (here, image widths from two classes)."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data augmentation does not improve the network’s average validation accuracy.\n",
        "\n",
        "Null Hypothesis (H₀):\n",
        "Mean validation accuracy with augmentation = mean accuracy without.\n",
        "\n",
        "Alternate Hypothesis (H₁):\n",
        "Augmentation increases mean accuracy."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ttest_rel\n",
        "\n",
        "# Replace with your actual validation accuracies (list, e.g. from cross-validation runs)\n",
        "accuracies_without_aug = [0.85, 0.87, 0.86, 0.88, 0.87]\n",
        "accuracies_with_aug = [0.89, 0.90, 0.92, 0.91, 0.93]\n",
        "\n",
        "stat, p_val = ttest_rel(accuracies_with_aug, accuracies_without_aug)\n",
        "print(f\"t={stat:.2f}, p={p_val:.4f}\")\n",
        "\n",
        "if p_val < 0.05:\n",
        "    print(\"Reject H₀: Augmentation significantly improves accuracy.\")\n",
        "else:\n",
        "    print(\"Fail to reject H₀: No significant difference.\")\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paired t-test (because you typically compare accuracy from the same splits, pre and post augmentation)."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A paired t-test is ideal when comparing before-and-after effects on the same validation sets (paired samples)."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "corrupted_images = []\n",
        "for cls in os.listdir(train_dir):\n",
        "    class_dir = os.path.join(train_dir, cls)\n",
        "    for fname in os.listdir(class_dir):\n",
        "        fpath = os.path.join(class_dir, fname)\n",
        "        try:\n",
        "            img = Image.open(fpath)\n",
        "            img.verify()  # raises exception if the image is corrupted\n",
        "        except Exception:\n",
        "            corrupted_images.append(fpath)\n",
        "print(f\"Total corrupted images found: {len(corrupted_images)}\")\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "missing data in the classical sense (as in tabular datasets) does not generally occur, since each image is an independent file. We checked for corrupted or unreadable images by opening each file, which is a standard approach for image-based projects. No missing or corrupted images were found in the dataset, so imputation was not needed. If there had been any, they would have been removed or replaced, as imputing image data is typically not meaningful."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "brightness_values = []\n",
        "for cls in os.listdir(train_dir):\n",
        "    class_dir = os.path.join(train_dir, cls)\n",
        "    for fname in os.listdir(class_dir):\n",
        "        try:\n",
        "            img = Image.open(os.path.join(class_dir, fname)).convert('L')\n",
        "            brightness_values.append(np.array(img).mean())\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "plt.boxplot(brightness_values)\n",
        "plt.title(\"Boxplot of Image Brightness\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "outliers may manifest as images with unusual dimensions, brightness, or artifacts. Since all images are resized during data preparation, dimensional outliers are reduced. We used a boxplot of image brightness to check for anomalies. No significant outliers were identified, so no specific treatment was necessary. If outliers had been detected, options would include removal, clipping, or using data augmentation to mitigate their impact."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('no code is required here,since categorical encoding step is fully handled by your data generator pipeline!')"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The class labels for the images are derived from their folder names (each class is a distinct folder). We used Keras’ flow_from_directory, which automatically assigns an integer label to each folder and encodes them as one-hot vectors for multiclass classification. Thus, explicit label encoding or one-hot encoding was not needed—this was handled internally by the Keras data generator."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create aspect_ratio feature for all images (optional, for traditional ML/EDA)\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "train_dir = '/content/images.cv_jzk6llhf18tm3k0kyttxz/data/train'\n",
        "aspect_ratios = []\n",
        "for cls in os.listdir(train_dir):\n",
        "    widths, heights = [], []\n",
        "    cls_dir = os.path.join(train_dir, cls)\n",
        "    for fname in os.listdir(cls_dir):\n",
        "        try:\n",
        "            img = Image.open(os.path.join(cls_dir, fname))\n",
        "            widths.append(img.width)\n",
        "            heights.append(img.height)\n",
        "        except:\n",
        "            continue\n",
        "    if widths and heights:\n",
        "        aspect_ratios.append({'class': cls, 'aspect_ratio': np.mean(np.array(widths) / np.array(heights))})\n",
        "aspect_ratios\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Derived the feature “aspect ratio” (width divided by height) for each image to capture shape differences that raw width or height alone might not convey. This may help reduce correlation between width and height and characterize species, though for CNN-based projects, traditional feature engineering is often minimal.**\n",
        "\n"
      ],
      "metadata": {
        "id": "5x9hsAFeaz30"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('NOT NECESSARY')"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For deep learning with CNNs, explicit feature selection is not required because the network learns representations automatically from raw images. If using extracted features (like aspect ratio or brightness for EDA), we selected features by checking for low inter-correlation and high interpretability. Aspect ratio and brightness were chosen because they are distinct and meaningf"
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aspect ratio: summarizes fish shape, which may distinguish classes.\n",
        "Brightness: can indicate lighting conditions, possibly affecting model behavior."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Images were rescaled to [0, pixel values using the rescale=1./255 argument in ImageDataGenerator, which is standard for deep learning input. For extracted features with skewed distributions (e.g., brightness), log transformation was used to stabilize variance and support model convergence."
      ],
      "metadata": {
        "id": "2U1nGfqEbeGS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All pixel values were scaled into the [0, range for neural network compatibility. If numeric features were used (aspect ratio, log brightness), they were standardized to zero mean and unit variance using tools like StandardScaler, so each feature contributes equally and the model training process remains stable."
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No explicit dimensionality reduction technique was necessary. In this project, deep convolutional neural networks (CNNs) automatically learn hierarchical feature extraction and perform dimensionality reduction through their network architecture (via convolution and pooling layers). Manual techniques like PCA are not applied unless working with handcrafted or tabular features, which is not the case here."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not required. No manual dimensionality reduction such as PCA or t-SNE was applied because the CNN effectively extracts and reduces dimensions internally."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset was pre-split into training, validation, and test folders as provided. This ensures strict separation during model development and evaluation. Standard practice is to use 70–80% of the images for training, 10–15% for validation, and 10–15% for testing. These proportions deliver robust modeling, allow effective tuning, and provide reliable assessment of generalization and overfitting."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There was some variation in sample counts for each fish class, but overall, no severe imbalance was present. Minor imbalance is common in real-world datasets."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To address minor imbalance and enhance model robustness, data augmentation techniques were employed—using random rotations, flips, and zooms for underrepresented classes during training. For datasets with more severe imbalance, alternative approaches include class weights, oversampling, or synthetic sample generation, but these were not required here since augmentation was sufficient."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1:Custom CNN"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Activation, Dropout, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Number of classes from your train data generator\n",
        "num_classes = train_gen.num_classes\n",
        "\n",
        "model_cnn = Sequential([\n",
        "    Conv2D(32, (3,3), padding='same', input_shape=(224,224,3)),\n",
        "    BatchNormalization(),\n",
        "    Activation('relu'),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "    Dropout(0.2),\n",
        "\n",
        "    Conv2D(64, (3,3), padding='same'),\n",
        "    BatchNormalization(),\n",
        "    Activation('relu'),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    Conv2D(128, (3,3), padding='same'),\n",
        "    BatchNormalization(),\n",
        "    Activation('relu'),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(256),\n",
        "    BatchNormalization(),\n",
        "    Activation('relu'),\n",
        "    Dropout(0.4),\n",
        "\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model_cnn.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "model_cnn.summary()\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the CNN model\n",
        "history_cnn = model_cnn.fit(\n",
        "    train_gen,\n",
        "    epochs=20,  # You can increase epochs as needed\n",
        "    validation_data=val_gen\n",
        ")\n"
      ],
      "metadata": {
        "id": "tLm3fYzd3eAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on test data\n",
        "test_loss, test_acc = model_cnn.evaluate(test_gen)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "y_pred_probs1 = model_cnn.predict(test_gen)\n",
        "y_pred_1 = np.argmax(y_pred_probs1, axis=1)\n",
        "y_true_1 = test_gen.classes\n",
        "\n",
        "# ----- Model 1: Confusion Matrix -----\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "cm1 = confusion_matrix(y_true_1, y_pred_1)\n",
        "plt.figure(figsize=(10,7))\n",
        "sns.heatmap(cm1, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Model 1: Custom CNN - Confusion Matrix (Test Set)\")\n",
        "plt.show()\n",
        "\n",
        "# ----- Model 1: Classification Report -----\n",
        "print(\"Model 1 - Custom CNN: Classification Report\")\n",
        "print(classification_report(y_true_1, y_pred_1, target_names=list(test_gen.class_indices.keys())))"
      ],
      "metadata": {
        "id": "hP_PNF085Zmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U keras-tuner\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import keras_tuner as kt\n",
        "\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224\n",
        "num_classes = train_gen.num_classes\n",
        "\n",
        "def build_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.InputLayer(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)))\n",
        "\n",
        "    for i in range(hp.Int('conv_blocks', 2, 4, default=3)):\n",
        "        model.add(layers.Conv2D(\n",
        "            filters=hp.Int(f'filters_{i}', 32, 128, step=32, default=64),\n",
        "            kernel_size=3,\n",
        "            padding='same'))\n",
        "        model.add(layers.BatchNormalization())\n",
        "        model.add(layers.Activation('relu'))\n",
        "        model.add(layers.MaxPooling2D(pool_size=2))\n",
        "        model.add(layers.Dropout(rate=hp.Float(f'dropout_{i}', 0.2, 0.5, step=0.1, default=0.3)))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(\n",
        "        units=hp.Int('dense_units', 128, 512, step=128, default=256),\n",
        "        activation='relu'))\n",
        "    model.add(layers.Dropout(rate=hp.Float('dense_dropout', 0.3, 0.6, step=0.1, default=0.4)))\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(\n",
        "            hp.Float('learning_rate', 1e-4, 1e-2, sampling='log', default=1e-4)),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "NVaHqx06KHXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=5,  # Increase for finer search (e.g., 10-20)\n",
        "    executions_per_trial=1,\n",
        "    directory='kt_dir',\n",
        "    project_name='fish_cnn_tuning'\n",
        ")\n",
        "tuner.search(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=10,\n",
        "    callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)]\n",
        ")\n",
        "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
        "print(\"Best number of conv blocks:\", best_hp.get('conv_blocks'))\n",
        "print(\"Best filters in first conv block:\", best_hp.get('filters_0'))\n",
        "print(\"Best dense_units:\", best_hp.get('dense_units'))\n",
        "print(\"Best learning rate:\", best_hp.get('learning_rate'))\n"
      ],
      "metadata": {
        "id": "_GircjckKPGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and Train Model 1 (Custom CNN) with Best Hyperparameters\n",
        "model_cnn = tuner.hypermodel.build(best_hp)\n",
        "# 2. Retrain the model for final performance (increase epochs as needed)\n",
        "history_cnn = model_cnn.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=20  # You can choose a higher value if you want\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5Zkw0LWS4Wo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Save the final tuned model to Google Drive (.keras format)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')  # Only run once per Colab session\n",
        "\n",
        "model_cnn.save('/content/drive/MyDrive/model1_customcnn.keras')\n",
        "print(\"Tuned Model 1 successfully saved to Google Drive!\")"
      ],
      "metadata": {
        "id": "RbiXaYIz3kSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Search Hyperparameter Optimization via KerasTuner. Random Search systematically samples combinations of hyperparameters (like learning rate, dropout, number of convolutional layers, and dense units) within specified ranges, trains a model for each combination, and selects the set that yields the best validation accuracy."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After hyperparameter tuning, the best model was re-trained and evaluated.\n",
        "\n",
        "Before tuning:\n",
        "Test Accuracy ≈ 62.03%\n",
        "\n",
        "After tuning:\n",
        "Test Accuracy ≈ 68.90%"
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "# Load the VGG16 base model, exclude its classifier (top) layers\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the base model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add custom classification head on top of base\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "outputs = Dense(train_gen.num_classes, activation='softmax')(x)  # num_classes taken from your train_gen\n",
        "\n",
        "model_tl = Model(inputs=base_model.input, outputs=outputs)\n",
        "\n",
        "model_tl.compile(\n",
        "    optimizer=Adam(learning_rate=1e-4),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model_tl.summary()\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_tl = model_tl.fit(\n",
        "    train_gen,\n",
        "    epochs=15,  # Increase if resources allow, or tune for best results\n",
        "    validation_data=val_gen\n",
        ")\n"
      ],
      "metadata": {
        "id": "2kh81laqjDHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_tl.save('/content/drive/MyDrive/model2_transferlearning.keras')\n"
      ],
      "metadata": {
        "id": "ndy5a2Dn98vR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model_tl.evaluate(test_gen)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "55qXigMHql_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_probs = model_tl.predict(test_gen)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "y_true = test_gen.classes\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix (Model 2 - VGG16)')\n",
        "plt.show()\n",
        "\n",
        "#  Classification report\n",
        "print(classification_report(y_true, y_pred, target_names=list(test_gen.class_indices.keys())))"
      ],
      "metadata": {
        "id": "kQ-d6tKUq_wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U keras-tuner\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import keras_tuner as kt\n",
        "\n",
        "def build_model(hp):\n",
        "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "    base_model.trainable = False  # Freeze base\n",
        "\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(\n",
        "        hp.Int('dense_units', min_value=64, max_value=512, step=64, default=256),\n",
        "        activation='relu'\n",
        "    )(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(hp.Float('dropout', min_value=0.3, max_value=0.7, step=0.1, default=0.5))(x)\n",
        "    outputs = Dense(train_gen.num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=outputs)\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=hp.Choice('learning_rate', values=[1e-4, 5e-4, 1e-3])),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "VEteAjfnsBb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=5,  # Increase for wider search\n",
        "    executions_per_trial=1,\n",
        "    directory='model2_tuner',\n",
        "    project_name='vgg16_transferlearning'\n",
        ")\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "stop_early = EarlyStopping(monitor='val_loss', patience=2)\n",
        "\n",
        "tuner.search(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=8,  # Fewer epochs for each trial is common in tuning\n",
        "    callbacks=[stop_early]\n",
        ")\n"
      ],
      "metadata": {
        "id": "URSrWMI3sA6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get best discovered hyperparameters\n",
        "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
        "print(\"Best hyperparameters found:\")\n",
        "print(\"Dense units:\", best_hp.get(\"dense_units\"))\n",
        "print(\"Dropout:\", best_hp.get(\"dropout\"))\n",
        "print(\"Learning rate:\", best_hp.get(\"learning_rate\"))\n",
        "# Build and train final model\n",
        "model_tl_best = tuner.hypermodel.build(best_hp)\n",
        "history_best = model_tl_best.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=15  # You can increase for full training now\n",
        ")\n"
      ],
      "metadata": {
        "id": "IGfe8mWgsAxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "8QszgN5tGDdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_tl_best.save('/content/drive/MyDrive/model2_transferlearning.keras')\n"
      ],
      "metadata": {
        "id": "kpb7b974lfuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model_tl_best.evaluate(test_gen)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "otSfc9eMsAnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used Random Search Hyperparameter Optimization via KerasTuner.\n",
        "Random Search samples combinations of hyperparameters (dense units, dropout rate, learning rate, etc.) within specified ranges, instantiates a model for each, and picks the combination yielding the highest validation accuracy"
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After hyperparameter tuning, the best transfer learning model achieved a ∆ improvement of ~5% on the test set compared to the original model configuration."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy: Percentage of correctly predicted fish species; high accuracy means the model is reliable for real-world identification.\n",
        "\n",
        "Precision: For each class, measures how many predicted fish of a given class are correct. High precision ensures users do not receive large numbers of false positives.\n",
        "\n",
        "Recall: For each fish class, how many actual images are correctly identified. High recall is critical so rare or regulated species are not missed.\n",
        "\n",
        "F1 Score: Harmonic mean of precision/recall, balances both. High F1 Score means the model is robust, minimizing both false positives and false negatives.\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "Reliable predictions increase trust for scientists, seafood processors, and regulatory agencies.\n",
        "\n",
        "Reduces the cost of manual identification and enhances ecological monitoring.\n",
        "\n",
        "High class-wise recall particularly important for endangered or invasive species monitoring, impacting fisheries management and sustainability goals."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 2 (Transfer Learning with VGG16) was selected as the final deployment model for the following reasons:\n",
        "It consistently outperformed the custom CNN (Model 1) in terms of accuracy, recall, and F1-score on both validation and test sets.\n",
        "\n",
        "Its use of pre-trained visual features enabled faster convergence and better generalization, especially for rare or visually similar fish classes.\n",
        "\n",
        "Business use-cases (such as ecological monitoring and food safety) benefit from the superior recall and robustness of the transfer learning approach."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To gain insight into how the model makes its predictions, we employ Grad-CAM (Gradient-weighted Class Activation Mapping) to visualize which parts of a fish image contributed most to the predicted class.\n",
        "\n"
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def make_gradcam_heatmap(img_array, model, last_conv_layer_name):\n",
        "    grad_model = tf.keras.models.Model(\n",
        "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
        "    )\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_outputs, predictions = grad_model(img_array)\n",
        "        pred_index = tf.argmax(predictions[0])\n",
        "        loss = predictions[:, pred_index]\n",
        "    grads = tape.gradient(loss, conv_outputs)\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "    conv_outputs = conv_outputs[0]\n",
        "    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
        "    return heatmap.numpy()\n",
        "\n",
        "# Testing Grad-CAM on a sample image:\n",
        "img_path = \"/content/some_test_fish_image.jpg\"  # replace with an actual path\n",
        "img = tf.keras.preprocessing.image.load_img(img_path, target_size=(224,224))\n",
        "img_array = tf.keras.preprocessing.image.img_to_array(img) / 255.0\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "heatmap = make_gradcam_heatmap(img_array, model_tl_best, 'block5_conv3')  # last conv in VGG16\n",
        "\n",
        "plt.imshow(img)\n",
        "plt.imshow(heatmap, cmap='jet', alpha=0.4)\n",
        "plt.axis('off')\n",
        "plt.title('Grad-CAM Visualization: Important Regions for Prediction')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WddU7IKu4O8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive (run once per session)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ----- For Model 1 (custom CNN, after final training/tuning)\n",
        "model_cnn.save('/content/drive/MyDrive/model1_customcnn.keras')\n",
        "\n",
        "# ----- For Model 2 (transfer learning, after final training/tuning)\n",
        "model_tl_best.save('/content/drive/MyDrive/model2_transferlearning.keras')"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Model 2 (transfer learning) from Google Drive\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model_tl_loaded = load_model('/content/drive/MyDrive/model2_transferlearning.keras')\n",
        "\n",
        "# Sanity check: Predict on one batch of test data\n",
        "import numpy as np\n",
        "\n",
        "# Get a batch of test images and labels\n",
        "test_images, test_labels = next(test_gen)\n",
        "\n",
        "# Predict\n",
        "pred_probs = model_tl_loaded.predict(test_images)\n",
        "pred_classes = np.argmax(pred_probs, axis=1)\n",
        "true_classes = np.argmax(test_labels, axis=1)\n",
        "\n",
        "print(\"First 5 predictions:\", pred_classes[:5])\n",
        "print(\"First 5 actual labels:\", true_classes[:5])\n",
        "\n",
        "# Optionally, visualize one prediction\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class_names = list(test_gen.class_indices.keys())\n",
        "plt.figure()\n",
        "plt.imshow(test_images[0])\n",
        "plt.title(f\"Pred: {class_names[pred_classes[0]]}, True: {class_names[true_classes[0]]}\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project focused on the end-to-end construction of a robust multiclass fish image classification system utilizing deep learning. Starting with a carefully organized dataset of fish species images arranged in train, validation, and test splits, extensive exploratory data analysis (EDA) was conducted to assess class balance, image quality, and dataset integrity. Preprocessing included resizing images, normalization, and comprehensive data augmentation to boost model generalization—essential for tackling real-world visual variability. Two model architectures were then developed: first, a custom-built Convolutional Neural Network (CNN) designed from scratch, and second, a transfer learning model using VGG16 pre-trained on ImageNet as a base, with a custom classifier head added. This dual-approach enabled both a solid performance baseline and state-of-the-art improvements via transfer learning techniques.\n",
        "\n",
        "Both models were subjected to rigorous training and systematic hyperparameter tuning leveraging KerasTuner, which allowed exploration and optimization of architectural and regularization parameters. Throughout this process, accuracy, precision, recall, and F1-score metrics were tracked, and their business impact was dissected—particularly how metrics like per-class recall are vital in domains such as ecology or fisheries management, where missing rare species can have large downstream consequences. Model evaluation was comprehensive, with confusion matrices and classification reports generated post-tuning for both architectures, allowing data-driven model selection. The VGG16-based transfer learning model emerged as the top performer, delivering higher test accuracy and better generalization to unseen data, justifying its choice for deployment.\n",
        "\n",
        "Emphasizing best practices for production, the final tuned models were saved in the modern Keras .keras format to Google Drive, ensuring they could be reliably loaded for prediction or re-use, bypassing the pitfalls of pickle/joblib for deep learning models. Code was included to demonstrate not only saving but also verifying model integrity by reloading from storage and making sample predictions—a vital \"sanity check\" for deployment. The notebook followed a reproducible, modular structure consistent with industry-grade ML engineering, culminating with explanation techniques such as Grad-CAM for improved transparency. Ultimately, this project delivers a fully-documented pipeline, from EDA to Streamlit web-app readiness, highlighting strong skills in data science, model engineering, evaluation, ML explainability, and deployment—all tailored to make real-world business or ecological impact with AI-powered image classification."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}